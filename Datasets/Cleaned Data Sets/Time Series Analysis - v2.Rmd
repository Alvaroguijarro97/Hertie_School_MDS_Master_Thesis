---
title: "Time series models"
author: "Alvaro Guijarro"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries, include=Fase}
library(readxl)
library(dplyr)
library(tidyr)
library(lubridate)
library(ggplot2)
library(forecast)
library(stats)
library(tseries)
library(skimr)
library(fmtr)
library(plotly)
library(corrplot)
library(tseries)
library(rvest)
```


```{r read datasets}
GEIH <- read_excel("GEIH_cleaned_2015-03-2023-12.xlsx")
Population <- read_excel("Population_cleaned_2010-12-2023-12.xlsx")
CPI <- read_excel("CPI_Information_cleaned_2015-01-2024-01.xlsx")
Education <- read_excel("Education_data_cleaned_2011-12-2022-12.xlsx")
Monetary_poverty <- read_excel("Monetary_Poverty_cleaned_2012-12-2022-12.xlsx")
cities_indicators <- read_excel("Cities_Indicators_cleaned_2016-12-2021-12.xlsx")
fiscal_performance <- read_excel("Fiscal_Performance_cleaned_2000-12-2022-12.xlsx")
latitude_longitude <- read_excel("Latitude_Longitude_cleaned.xlsx")
score_fiscal_performance <- read_excel("Score_Fiscal_Performance_cleaned_2015-12-2022-12.xlsx")
```


```{r proof they have the necessary information}
head(GEIH)
head(Population)
head(CPI)
head(Education)
head(Monetary_poverty)
head(cities_indicators)
head(fiscal_performance)
head(latitude_longitude)
head(score_fiscal_performance)
```
Let's look more into each dataset datatype and summary
```{r}
skim(GEIH)
skim(Population)
skim(CPI)
skim(Education)
skim(Monetary_poverty)
skim(cities_indicators)
skim(fiscal_performance)
skim(latitude_longitude)
skim(score_fiscal_performance)
```

Let's make sure all of the columns are in the same format in order to perform the join. 
```{r}
GEIH$date <- format(as.Date(GEIH$date), "%Y-%m")
GEIH$city <- as.character(GEIH$city)

Population$date <- format(as.Date(Population$date), "%Y-%m")
Population$city <- as.character(Population$city)


summary(GEIH)
summary(Population)
```
Let's filter our GEIH data so we only have the 13 cities we will look into out analysis. 
```{r}
print("Initial GEIH unique cities")
unique(GEIH$city)
print("Initial Population unique cities")
unique(Population$city)
GEIH_c <- GEIH %>%
  filter(city  %in% c("BARRANQUILLA A.M.","BOGOTÁ D.C.","BUCARAMANGA A.M.","CALI A.M.","CARTAGENA","CÚCUTA A.M.","IBAGUÉ","MANIZALES A.M.","MEDELLÍN A.M.","MONTERÍA","PASTO","PEREIRA A.M.","VILLAVICENCIO"))

print("Cities of interest for the GEIH")
unique(GEIH_c$city)
```

Let's add a tag to each variable to identify from which dataset they came from
```{r}
# Function to rename columns with a suffix, excluding specified columns
rename_with_suffix <- function(df, suffix, exclude = c()) {
  df %>% rename_with(~paste0(., suffix), -all_of(exclude))
}

GEIH_c <- rename_with_suffix(GEIH_c, ".geih", exclude = c("city", "date"))
Population <- rename_with_suffix(Population, ".pop", exclude = c("city", "date"))
CPI <- rename_with_suffix(CPI, ".cpi", exclude = c("city", "date"))
Education <- rename_with_suffix(Education, ".edu", exclude = c("city", "date"))
Monetary_poverty <- rename_with_suffix(Monetary_poverty, ".mp", exclude = c("city", "date"))
cities_indicators <- rename_with_suffix(cities_indicators, ".ci", exclude = c("city", "date"))
fiscal_performance <- rename_with_suffix(fiscal_performance, ".fp", exclude = c("city", "date"))
latitude_longitude <- rename_with_suffix(latitude_longitude, ".ll", exclude = c("city"))
score_fiscal_performance <- rename_with_suffix(score_fiscal_performance, ".sfp", exclude = c("city", "date"))

```

What time frames are we working for each dataset? 
```{r}
# Function to get the date range of a dataset
get_date_range <- function(df, date_column) {
  range(df[[date_column]], na.rm = TRUE)
}

# Applying the function to each dataset and printing the date ranges
GEIH_date_range <- get_date_range(GEIH_c, "date")
Population_date_range <- get_date_range(Population, "date")
CPI_date_range <- get_date_range(CPI, "date")
Education_date_range <- get_date_range(Education, "date")
Monetary_poverty_date_range <- get_date_range(Monetary_poverty, "date")
cities_indicators_date_range <- get_date_range(cities_indicators, "date")
fiscal_performance_date_range <- get_date_range(fiscal_performance, "date")
score_fiscal_performance_date_range <- get_date_range(score_fiscal_performance, "date")

# Print the date ranges
print(paste("GEIH_c date range:", GEIH_date_range[1], "/", GEIH_date_range[2]))
print(paste("Population date range:", Population_date_range[1], "/", Population_date_range[2]))
print(paste("CPI date range:", CPI_date_range[1], "/", CPI_date_range[2]))
print(paste("Education date range:", Education_date_range[1], "/", Education_date_range[2]))
print(paste("Monetary_poverty date range:", Monetary_poverty_date_range[1], "/", Monetary_poverty_date_range[2]))
print(paste("cities_indicators date range:", cities_indicators_date_range[1], "/", cities_indicators_date_range[2]))
print(paste("fiscal_performance date range:", fiscal_performance_date_range[1], "/", fiscal_performance_date_range[2]))
print(paste("score_fiscal_performance date range:", score_fiscal_performance_date_range[1], "/", score_fiscal_performance_date_range[2]))

```

In Colombia, administrative periods for Mayors and Governors are for 4 years since 2008. Our datasets span between several administrative time periods (2008-2011 / 2012-2015 / 2016-2019 / 2020-2023 / 2024-2027), taking into consideration the 2020 Covid-19 pandemic and the disruption that brought to global health, economic, logistical, and productive systems, as well as the introduciton of the MDM statistic in 2016 ("Medición de Desempeño Municipal" = Municipal Performance Measurement, which ranks Colombian cities by their performance on various key economic, health, safety, and demographic indicators) we will be working with the *2016-2019* administrative time period for our prediction. 
```{r}
# Perform left joins to combine datasets based on 'city' and 'date'
df_analysis <- GEIH_c %>%
  left_join(Population, by = c("city", "date"), suffix = c("", ".pop")) %>%
  left_join(CPI, by = c("city", "date"), suffix = c("", ".cpi")) %>%
  left_join(Education, by = c("city", "date"), suffix = c("",".edu")) %>%
  left_join(Monetary_poverty, by = c("city", "date"), suffix = c("",".mp")) %>%
  left_join(cities_indicators, by = c("city", "date"), suffix = c("",".ci")) %>%
  left_join(fiscal_performance, by = c("city", "date"), suffix = c("",".fp")) %>%
  left_join(latitude_longitude, by = c("city"), suffix = c("",".ll")) %>%
  left_join(score_fiscal_performance, by = c("city", "date"), suffix = c("",".sfp")) %>%
  filter(date >= "2016-12" , date <= "2019-12")

unique(df_analysis$city)
unique(df_analysis$date)

skim(df_analysis)
summary(df_analysis)
```
```{r}
unique(df_analysis$Concepto.geih)

# List of unique worker types in the Concepto.geih column
worker_types <- unique(df_analysis$Concepto.geih)

economic_activities <- c(
  "Ocupados" = "Employed",
  "No informa" = "Not Reported",
  "Agricultura, ganadería, caza, silvicultura y pesca" = "Agriculture, Livestock, Hunting, Forestry, and Fishing",
  "Explotación de minas y canteras" = "Mining and Quarrying",
  "Industrias manufactureras" = "Manufacturing Industries",
  "Suministro de electricidad gas, agua y gestión de desechos" = "Electricity, Gas, Water Supply, and Waste Management",
  "Construcción" = "Construction",
  "Comercio y reparación de vehículos" = "Commerce and Vehicle Repair",
  "Alojamiento y servicios de comida" = "Accommodation and Food Services",
  "Transporte y almacenamiento" = "Transportation and Storage",
  "Información y comunicaciones" = "Information and Communications",
  "Actividades financieras y de seguros" = "Financial and Insurance Activities",
  "Actividades inmobiliarias" = "Real Estate Activities",
  "Actividades profesionales, científicas, técnicas y servicios administrativos" = "Professional, Scientific, Technical Activities, and Administrative Services",
  "Administración pública y defensa, educación y atención de la salud humana" = "Public Administration and Defense, Education, and Human Health Care", 
  "Actividades artísticas, entretenimiento recreación y otras actividades de servicios" = "Artistic Activities, Entertainment, Recreation, and Other Service Activities")

df_analysis_eng <- df_analysis %>% 
  mutate(Concepto.geih = recode(Concepto.geih, !!!economic_activities))

unique(df_analysis_eng$Concepto.geih)

str(df_analysis_eng)
colnames(df_analysis_eng)
```
Let's only select the colums that are of our interest:

```{r}
df_analysis_selected <- df_analysis_eng %>% 
  select(-city_id.pop,-city_id.edu,-year.edu,-month.edu,-year.mp,-month.mp,-year.ci,-month.ci,-year.fp,-month.fp,-year.sfp,-month.sfp,-capital.ll) %>%
  rename( "eco_activity" = "Concepto.geih", 
          "year" = "year.pop",
          "month" = "month.pop",
          "CPI_YTD.cpi" ="CPI_year_to_date_var.cpi")
str(df_analysis_selected)

#Let's add a description of what each variables means and in what unit they are stored, in order to preprocess it before setting up our prediction models.
descriptions(df_analysis_selected) <- list(
  city = "Name of a city in Colombia",
  eco_activity= "Type of Economic Activity under CIIU 4 A.C",
  date = "date - Year and Month",
  workers.geih = "population - Employed population",
  population_month.pop = "population - Total Population in monthly frecuency (interpolated)",
  year = "date - Year",
  month = "date - Month",
  population_year.pop = "population - Total Population in yearly frecuency",
  CPI.cpi = "Consumer Price Index, The Consumer Price Index (CPI) is a measure that examines the weighted average of prices of a basket of consumer goods and services, such as transportation, food, and medical care. The CPI is calculated by taking price changes for each item in the predetermined basket of goods and averaging them. ",
  CPI_YTD.cpi = "% variance -  The CPI (Consumer Price Index) year-to-date (YTD) variance refers to the change in the CPI from the beginning of the current year up to a specific point in time within the same year. ",
  CPI_year_var.cpi = "% variance - The CPI (Consumer Price Index) yearly variance refers to the percentage change in the CPI over a 12-month period. It measures the rate of inflation or deflation by comparing the price level of the CPI at the end of a year to the price level at the end of the previous year.",
  CPI_month_var.cpi = "% variance - The CPI (Consumer Price Index) monthly variance refers to the change in the CPI from one month to the next, expressed as a percentage. This measure provides an indication of how consumer prices have moved within a month, reflecting short-term inflation or deflation trends.",
  Enrollment_Rate_5_16.edu = "% - Proportion of the population between 5 and 16 years old who are attending the educational system. When DANE's population projections do not adequately capture internal migratory flows, it can reach values greater than 100%.",
  Net_Coverage.edu = "% - It is the ratio between the number of students enrolled in transition, primary, secondary, and high school who have the theoretical age (5 to 16 years) and the total population of that same age. When DANE's population projections do not adequately capture internal migratory flows, it can reach values greater than 100%.",
  Net_Coverage_Transition.edu = "% - It is the ratio between the number of students enrolled in transition who have the theoretical age to attend this level (5 years) and the total population of that same age. When DANE's population projections do not adequately capture internal migratory flows, it can reach values greater than 100%.",
  Net_Coverage_Primary.edu = "% - It is the ratio between the number of students enrolled in primary who have the theoretical age to attend this level (6 to 10 years) and the total population of that same age. When DANE's population projections do not adequately capture internal migratory flows, it can reach values greater than 100%.",
  Net_Coverage_Secondary.edu = "% - It is the ratio between the number of students enrolled in secondary who have the theoretical age to attend this level (11 to 14 years) and the total population of that same age. When DANE's population projections do not adequately capture internal migratory flows, it can reach values greater than 100%.",
  Net_Coverage_HighSchool.edu = "% - # It is the ratio between the number of students enrolled in high school who have the theoretical age to attend this level (15 to 16 years) and the total population of that same age. When DANE's population projections do not adequately capture internal migratory flows, it can reach values greater than 100%.",
  Dropout_Rate.edu = "% - # Intra-annual dropout rate of the official sector. Identifies the proportion of enrolled students who, due to cultural factors, conjunctural situations, or the provision of educational service, leave their studies during the academic year.",
  Dropout_Rate_Transition.edu = "% - Intra-annual dropout rate of the official sector in transition. Identifies the proportion of enrolled students who, due to cultural factors, conjunctural situations, or the provision of educational service, leave their studies during the academic year.",
  Dropout_Rate_Primary.edu = "% - Intra-annual dropout rate of the official sector in primary. Identifies the proportion of enrolled students who, due to cultural factors, conjunctural situations, or the provision of educational service, leave their studies during the academic year.",
  Dropout_Rate_Secondary.edu = "% - Intra-annual dropout rate of the official sector in secondary. Identifies the proportion of enrolled students who, due to cultural factors, conjunctural situations, or the provision of educational service, leave their studies during the academic year.",
  Dropout_Rate_HighSchool.edu = "% - Intra-annual dropout rate of the official sector in high school. Identifies the proportion of enrolled students who, due to cultural factors, conjunctural situations, or the provision of educational service, leave their studies during the academic year.",
  Pass_Rate.edu = "% - Pass rate of students in the official sector. Identifies the percentage of students in preschool, basic, and high school education who pass according to current educational plans and programs.",
  Pass_Rate_Transition.edu = "% - Pass rate of students in the official sector in transition. Identifies the percentage of students at this educational level who pass according to current educational plans and programs.",
  Pass_Rate_Primary.edu = "% - Pass rate of students in the official sector in primary. Identifies the percentage of students at this educational level who pass according to current educational plans and programs.",
  Pass_Rate_Secondary.edu = "% - Pass rate of students in the official sector in secondary. Identifies the percentage of students at this educational level who pass according to current educational plans and programs.",
  Pass_Rate_HighSchool.edu = "% - Pass rate of students in the official sector in high school. Identifies the percentage of students at this educational level who pass according to current educational plans and programs.",
  Fail_Rate.edu = "% - Failure rate of students in the official sector. Identifies the percentage of students in preschool, basic, and high school education who fail according to current educational plans and programs.",
  Fail_Rate_Transition.edu = "% - Failure rate of students in the official sector in transition. Identifies the percentage of students at this educational level who fail according to current educational plans and programs.",
  Fail_Rate_Primary.edu = "% - Failure rate of students in the official sector in primary. Identifies the percentage of students at this educational level who fail according to current educational plans and programs.",
  Fail_Rate_Secondary.edu = "% - Repetition rate of the official sector. Corresponds to the percentage of students enrolled in secondary education who are repeating the same grade as the previous year.",
  Fail_Rate_HighSchool.edu = "% - Repetition rate of the official sector. Corresponds to the percentage of students enrolled in high school who are repeating the same grade as the previous year.",
  I_PM.mp = "% of population - Monetary Poverty Rate",
  I_PME.mp = "% of population - Extreme Monetary Poverty Rate",
  Gini.mp = "Gini Coeficient (values between 0-1)",
  IPUG.mp = "$COP Values in Current Pesos - Average Per Capita Income of the Household Spending Unit",
  LP.mp = "$COP Values in Current Pesos - Monetary Poverty Lines (monthly values per person)",
  LPE.mp = "$COP Extreme Monetary Poverty Lines (monthly values per person), Values in Current Pesos",
  MDM_Resource_Mobilization.ci = "Score between 1-100 - Measures mobilization of financial resources",
  Tax_And_Non_Tax_Revenue_Per_Capita.ci = "$ COP Values in Current Pesos - Tax and non-tax revenue per capita, excluding territorial order collections",
  Revenue_From_OT_Instruments_Per_Capita.ci = "$ COP Values in Current Pesos - Revenue collected through territorial ordering instruments per capita",
  Investment_Financed_By_Own_Resources.ci = "% - Percentage of investment financed by the municipality's own resources",
  MDM_Execution_Of_Resources.ci = "Score between 1-100 - Execution of financial resources",
  MDM_Open_Government_And_Transparency.ci = "Score between 1-100 - Measures of open government and transparency practices",
  MDM_Territorial_Ordering.ci = "Score between 1-100 - Territorial ordering and planning measures",
  Effective_Collection_Rate.ci = "Effective rate of tax collection",
  MDM_Education.ci = "Score between 1-100 - Educational coverage and quality in middle education",
  MDM_Health_Coverage.ci = "Score between 1-100 - Health coverage and services",
  Health_Coverage_Overall.ci = "% of Population - Overall health coverage from the affiliate registry",
  Pentavalent_Vaccination_Coverage.ci = "% of Populaion - Coverage rate of the pentavalent vaccine in infants",
  Infant_Mortality_Rate.ci = "# of infant deaths - Infant mortality rate per 1,000 live births",
  MDM_Services.ci = "Score 1-100 - Coverage and quality of public services",
  Rural_Electrical_Coverage.ci = "% of Population - Coverage of rural electrical service",
  Broadband_Penetration.ci = "% of Population - Number of broadband Internet subscribers relative to the total population",
  Aqueduct_Coverage.ci = "% of Populaion - Coverage of aqueduct water service",
  Sewerage_Coverage.ci = "% of Population - Coverage of sewerage service",
  MDM_Security_And_Coexistence.ci = "Score 1-100 - Security and social coexistence indicators",
  Theft_Rate_Per_10k_Inhabitants.ci = "# Reported theft cases per 10,000 inhabitants",
  Homicide_Rate_Per_10k_Inhabitants.ci = "# Homicide cases per 10,000 inhabitantsHomicide cases per 10,000 inhabitants",
  Domestic_Violence_Rate_Per_10k_Inhabitants.ci = "# of Domestic violence cases per 10,000 inhabitants",
  TotalIncome.fp = "$ Millions of Pesos - Total income received.",
  CurrentIncome.fp = "$ Millions of Pesos - Current (or operational) income.",
  TaxIncome.fp = "$ Millions of Pesos - Income received from taxes.",
  PropertyTax.fp = "$ Millions of Pesos - Property tax income.",
  IndustryAndCommerceTax.fp = "$ Millions of Pesos - Tax from industry and commerce activities.",
  FuelSurcharge.fp = "$ Millions of Pesos - Surcharge on fuel.",
  OtherTaxIncome.fp = "$ Millions of Pesos - Other tax-related income.",
  NonTaxIncome.fp = "$ Millions of Pesos - Non-tax related income.",
  CurrentTransfers.fp = "$ Millions of Pesos - Current transfers received.",
  NationalLevelCurrentTransfers.fp = "$ Millions of Pesos - Current transfers from the national level.",
  OtherTransfers.fp = "$ Millions of Pesos - Other transfers.",
  TotalExpenses.fp = "$ Millions of Pesos - Total expenses.",
  CurrentExpenses.fp = "$ Millions of Pesos - Current (or operational) expenses.",
  OperatingExpenses.fp = "$ Millions of Pesos - Operating expenses.",
  PersonalServices.fp = "$ Millions of Pesos - Expenses on personal services.",
  GeneralExpenses.fp = "$ Millions of Pesos - General expenses.",
  TransfersPaid.fp = "$ Millions of Pesos - Transfers paid out.",
  PublicDebtInterests.fp = "$ Millions of Pesos - Interests on public debt.",
  CurrentDissaving_Saving.fp = "$ Millions of Pesos - Current dissaving or saving.",
  CapitalIncome.fp = "$ Millions of Pesos - Income from capital.",
  Royalties.fp = "$ Millions of Pesos - Income from royalties.",
  NationalTransfers.fp = "$ Millions of Pesos - Transfers from the national level.",
  "Co-financing.fp" = "$ Millions of Pesos - Co-financing.",
  OtherCapitalIncome.fp = "$ Millions of Pesos - Other capital income.",
  CapitalExpenses.fp = "$ Millions of Pesos - - Capital expenses.",
  GrossCapitalFormation.fp = "$ Millions of Pesos - Gross capital formation.",
  OtherCapitalExpenses.fp = "$ Millions of Pesos - Other capital expenses.",
  TotalDeficitOrSurplus.fp = "$ Millions of Pesos - Total deficit or surplus.",
  FINANCING.fp = "$ Millions of Pesos - Financing.",
  NetCredit.fp = "$ Millions of Pesos - Net credit.",
  Disbursements.fp = "$ Millions of Pesos - Disbursements.",
  Amortizations.fp = "$ Millions of Pesos - Amortizations.",
  BalanceResources_VariationInDepositsAndOthers.fp = "$ Millions of Pesos - Balance resources, variation in deposits, and others.",
  lat.ll = "Latidude",
  lng.ll = "Longitud",
  lat_z.ll = "Z - transformation of Latitude",
  lng_z.ll = "Z - transformation of Longitute",
  "Self-financing_of_operating_expenses.sfp" = "Score 1-100 - Self-financing of operating expenses: the ability to cover the operating expenses of the central administration with unrestricted income (Law 617 of 2000) ",
  Debt_service_support.sfp = "Score 1-100 - Debt service support: the ability to support debt service with perceived revenues.",
  Dependence_on_transfers_from_the_Nation_and_Royalties.sfp = "Score 1-100 - Dependence on transfers from the Nation and Royalties: measures the importance of national transfers and royalties (SGR) in total revenues.",
  Generation_of_Own_Resources.sfp = "Score 1-100 - Generation of Own Resources: the ability to generate resources complementary to the transfers.",
  Magnitude_of_Investment.sfp = "Score 1-100 - Magnitude of Investment: quantifies the magnitude of the investment executed by the territorial entity.",
  Saving_Capacity.sfp = "Score 1-100 - Saving Capacity: determines the degree to which surpluses are freed up to finance investment.", 
  Fiscal_Performance_Indicator.sfp = "Score 1-100 - Fiscal Performance Indicator",
  Category.sfp = "Category - Type of Fiscar Performance of city "                                         
)
```

Let's run a tests on a subset of our data, only focusing on the total workers in the Colombian economy (where "eco_activity" = Employed), and only choose the variables we believe are going to have the most impact on our analysis

```{r}
df_analysis_selected$month <- as.numeric(df_analysis_selected$month)
employed_data <- df_analysis_selected %>% 
  filter(eco_activity == "Employed") %>%
#  select(-eco_activity,
#         -Net_Coverage_Transition.edu,-Net_Coverage_Primary.edu,-Net_Coverage_Secondary.edu,-Net_Coverage_HighSchool.edu,
#         -Dropout_Rate_Transition.edu,-Dropout_Rate_Primary.edu,-Dropout_Rate_Secondary.edu,-Dropout_Rate_HighSchool.edu,
#         -Pass_Rate_Transition.edu,-Pass_Rate_Primary.edu,-Pass_Rate_Secondary.edu,-Pass_Rate_HighSchool.edu,
#         -CurrentIncome.fp, -TaxIncome.fp,-PropertyTax.fp,-IndustryAndCommerceTax.fp,-FuelSurcharge.fp,-OtherTaxIncome.fp,
#         -OtherTaxIncome.fp,-NonTaxIncome.fp,-CurrentTransfers.fp,-NationalLevelCurrentTransfers.fp,-OtherTransfers.fp,
#         -CurrentExpenses.fp,-OperatingExpenses.fp,-PersonalServices.fp,-GeneralExpenses.fp,-TransfersPaid.fp,-PublicDebtInterests.fp,
#         -Royalties.fp,-NationalTransfers.fp,-"Co-financing.fp",-OtherCapitalIncome.fp, -GrossCapitalFormation.fp, -OtherCapitalExpenses.fp,
#         -NetCredit.fp,-Disbursements.fp,-Amortizations.fp,-BalanceResources_VariationInDepositsAndOthers.fp)
  select(workers.geih, city, eco_activity, date, year, month, population_month.pop, month,population_year.pop, CPI.cpi, CPI_month_var.cpi, Enrollment_Rate_5_16.edu, Net_Coverage.edu, Dropout_Rate.edu, Pass_Rate.edu, Fail_Rate.edu, I_PM.mp, I_PME.mp, Gini.mp, IPUG.mp, MDM_Resource_Mobilization.ci, MDM_Execution_Of_Resources.ci, MDM_Open_Government_And_Transparency.ci, MDM_Territorial_Ordering.ci, MDM_Education.ci, MDM_Health_Coverage.ci, Health_Coverage_Overall.ci, MDM_Services.ci, MDM_Security_And_Coexistence.ci, lat_z.ll, lng_z.ll, "Self-financing_of_operating_expenses.sfp", Debt_service_support.sfp, Dependence_on_transfers_from_the_Nation_and_Royalties.sfp , Generation_of_Own_Resources.sfp, Magnitude_of_Investment.sfp, Saving_Capacity.sfp, Fiscal_Performance_Indicator.sfp)
employed_data
```
Let's run some tests
```{r}
# 1. Trend Analysis by City
plotly_obj <- plot_ly(data = employed_data, x = ~date, y = ~workers.geih, 
                      color = ~city, colors = RColorBrewer::brewer.pal(n = 8, name = "Dark2"),
                      type = 'scatter', mode = 'lines+markers',
                      text = ~city, hoverinfo = 'text+x+y') %>%
  layout(title = "Total Number of Workers Over Time by City from 2016 to 2019",
         xaxis = list(title = "Year"),
         yaxis = list(title = "Number of Workers"), 
         legend = list(orientation = "v", x = 1.05, y = 1))

# Display the interactive plot
plotly_obj
```


For trials, let's do a model analysis for one city only. We'll use Bogota as our first option for the analysis.
```{r}
bogota_dataset <- employed_data %>%
  filter( employed_data$city == "BOGOTÁ D.C.") %>%
  select(-city, -eco_activity, -lat_z.ll, -lng_z.ll)
bogota_dataset
```



```{r}
bogota_dataset$date <- as.Date(paste0(bogota_dataset$date, "-01"), format = "%Y-%m-%d")
plot(bogota_dataset$date, bogota_dataset$workers.geih, type='l', main='Time Series Plot', xlab='Date', ylab='Number of Workers')
```
There seems to be some patterns that repeat each year, we need to run Stationarity and seasonality check
```{r}
# Split the data into two sections and compare means and variances
split1 <- bogota_dataset$workers.geih[1:(length(bogota_dataset$workers.geih)/2)]
split2 <- bogota_dataset$workers.geih[(length(bogota_dataset$workers.geih)/2 + 1):length(bogota_dataset$workers.geih)]
mean(split1)
mean(split2)
var(split1)
var(split2)

```
In order to determine whether the time series is stationary or non-stationary we can run the Augmented Dickey-Fuller (ADF) Test which is used to test the null hypothesis that there is a unit root present in the series, which suggests non-stationarity.
```{r}
adf.test(bogota_dataset$workers.geih, alternative="stationary")
```
The Dickey-Fuller statistic is -2.237.
The p-value is 0.4808.

Because the p-value is greater than 0.05, we fail to reject the null hypothesis at the 5% significance level. This means there is not enough statistical evidence to conclude that the time series is stationary. We will have to transform our data in order to make it stationary. 

Let's Identify the different seasonality components of our data to see what type of prediction model would be the most appropriate.
```{r}
decomposed <- stl(ts(bogota_dataset$workers.geih, frequency=12), s.window="periodic")
plot(decomposed)
```
From this seasonal decomposition analysis of the data from the city of Bogota we can conclude the following points:
- Observing the top "data" panel we can see that it has some level of fluctuation, possibly indicating seasonality.
- "seasonal" panel, the repeating patter suggests that there is a seasonal component in the employment data for the city. 
- "trend" panel shows a slight upward trend in the # of employed workers over time
- the "remainder" (residuals) panel show some spikes in the data, which could indicate either outliers or unexplained variance by the seasonal and trend components on our data. 

```{r}
seasonplot(ts(bogota_dataset$workers.geih, frequency=12))
```
```{r}
acf(bogota_dataset$workers.geih)
pacf(bogota_dataset$workers.geih)
```
Let's modify our predicting variable and prepare our data for modeling


```{r}
# Step 1) Apply the log transformation
bogota_dataset$log_workers <- log(bogota_dataset$workers.geih)

# Create the 'diff_log_workers' column with the correct length
bogota_dataset <- bogota_dataset %>%
  mutate(diff_log_workers = c(NA, diff(log_workers)))

# Step 2: Plot the differenced data, excluding the first NA value
with(bogota_dataset, {
  plot(date[-1], diff_log_workers[-1], type='l', 
       main='Differenced Log Transformed Number of Workers', 
       xlab='Date', ylab='Differenced Log of Workers')
})

# Step 3: Seasonal Decomposition, excluding the first NA value
decomposed <- stl(ts(bogota_dataset$diff_log_workers[-1], frequency = 12), s.window = "periodic")
plot(decomposed)

# Step 4: Stationarity Test on the differenced data, excluding the first NA value
# Make sure to install and library the tseries package if not already done
adf_test_result <- adf.test(na.omit(bogota_dataset$diff_log_workers), alternative = "stationary")
print(adf_test_result)


```
The results of the ADF Test suggest that the differnced log-transformed number of workers is stationary, as the p-value is below the significance level of 0.05. Since we have confirmed stationarity post-differencing and seasonality from the decomposition, le's build a SARIMA model to predict this variable. 

```{r}
# Check the ADF test for the other predictive variables
predictor_columns <- setdiff(names(bogota_dataset), c("workers.geih", date, year, month))

# Initialize a list to store ADF test results
adf_results <- list()

# Loop through predictors and perform ADF test
for (col in predictor_columns) {
  # Assuming the predictors are already numeric, else convert or exclude non-numeric
  time_series <- na.omit(bogota_dataset[[col]])
  
  # Only proceed if there's enough data after omitting NAs
  if (length(time_series) > 0) {
    adf_test_result <- adf.test(time_series, alternative = "stationary")
    adf_results[[col]] <- adf_test_result$p.value
  } else {
    adf_results[[col]] <- NA  # Not enough data to test
  }
}

# Check results
adf_results
```







OLS Model pipeline.We will start by checking for stationarity on our dataset by running the Augmented Dickey-Fuller test (ADF), which tests the null hypothesis that a unit root is present in a time series sample. 

```{r}
#Checking for stationarity
adf_result <- adf.test(bogota_dataset_ts, alternative = "stationary")
adf_result
```
he ADF test has a p-value of 0.4808, which is greater than the common alpha level of 0.05. This means that the null hypothesis of the presence of a unit root (non-stationarity) cannot be rejected for the employment data in Bogotá. This means that we need to tackle the presence of seasonality and a lack of stationarity in our time series, and for this, a model that can handle seasonality is more in line with our research quesiton. We will proceed to explore a SARIMA / ARIMA route of our prediction. 

Correlation Matrix 
```{r}
unique(colnames(bogota_dataset))

interesting_vars <- c("workers.geih", "MDM_Resource_Mobilization.ci" ,
"MDM_Execution_Of_Resources.ci" ,
"MDM_Open_Government_And_Transparency.ci" ,
"MDM_Territorial_Ordering.ci" ,
"MDM_Education.ci" ,
"MDM_Health_Coverage.ci" ,
"Health_Coverage_Overall.ci" ,
"MDM_Services.ci" ,
"MDM_Security_And_Coexistence.ci")

# Let's build a correlation matrix to get a view on co-dependency
cor_matrix <- cor(bogota_dataset[, interesting_vars], use = "complete.obs")
print(cor_matrix)
# Visual analysis
pairs(bogota_dataset[, interesting_vars], pch = 21, bg = c("red"))
```


```{r}
vif_model <- lm(workers.geih  ~  MDM_Resource_Mobilization.ci+ MDM_Execution_Of_Resources.ci+ MDM_Open_Government_And_Transparency.ci+ MDM_Territorial_Ordering.ci+ MDM_Education.ci+ MDM_Health_Coverage.ci+ Health_Coverage_Overall.ci+ MDM_Services.ci+ MDM_Security_And_Coexistence.ci, data = bogota_dataset)
vif(vif_model)
```


```{r}
# Split the data into training and test sets
bogota_dataset$date <- as.Date(paste0(bogota_dataset$date, "-01"), format = "%Y-%m-%d")

train <- bogota_dataset[bogota_dataset$date < "2019-06-01", ]
test <- bogota_dataset[bogota_dataset$date >= "2019-06-01", ]

# Build the univariate model using the historical 'workers' data
uni_model <- auto.arima(train$workers.geih)

# Forecast the 'workers' for 2019-06
uni_forecast <- forecast(uni_model, h = nrow(test))

# Build the multivariate model
multi_model <- lm(workers.geih ~ year + month + population_month.pop + population_year.pop + 
                  CPI.cpi + CPI_month_var.cpi + Enrollment_Rate_5_16.edu + Net_Coverage.edu + 
                  Dropout_Rate.edu + Pass_Rate.edu + Fail_Rate.edu + I_PM.mp + I_PME.mp + 
                  Gini.mp + IPUG.mp + MDM_Resource_Mobilization.ci + MDM_Execution_Of_Resources.ci + 
                  MDM_Open_Government_And_Transparency.ci + MDM_Territorial_Ordering.ci + 
                  MDM_Education.ci + MDM_Health_Coverage.ci + Health_Coverage_Overall.ci + 
                  MDM_Services.ci + MDM_Security_And_Coexistence.ci + lat_z.ll + lng_z.ll + 
                  `Self-financing_of_operating_expenses.sfp` + Debt_service_support.sfp + 
                  Dependence_on_transfers_from_the_Nation_and_Royalties.sfp + 
                  Generation_of_Own_Resources.sfp + Magnitude_of_Investment.sfp + 
                  Saving_Capacity.sfp + Fiscal_Performance_Indicator.sfp, 
                  data = train)

# Forecast 'workers' for 2019 using the multivariate model
# For multivariate models, we don't have a direct function like forecast(), so we have to predict manually.
test$predicted_workers <- predict(multi_model, newdata = test)

# Evaluate the models
# For the univariate model
accuracy(uni_forecast, test$workers.geih)

# For the multivariate model
multi_accuracy <- with(test, mean((workers.geih - predicted_workers)^2)) # Mean Squared Error

# Visualization
plot(test$date, test$workers.geih, type = "l", col = "blue", xlab = "Date", ylab = "Workers", main = "Actual vs Predicted Workers for Bogota in 2019")
lines(test$date, uni_forecast$mean, col = "red")
lines(test$date, test$predicted_workers, col = "black")
legend("topleft", legend = c("Actual", "Univariate", "Multivariate"), col = c("blue", "red", "black"), lty = 1)

```


```{r}
library(forecast)

# Identify potential models using auto.arima which automatically selects the best (p,d,q)(P,D,Q)[m]
sarima_model <- auto.arima(bogota_dataset_ts)

# Check model summary
summary(sarima_model)

# Forecast future points
forecasts <- forecast(sarima_model, h = 12)  # Forecast next 12 months
plot(forecasts)

```






PREVIOUS CODE


We have too many variables for the number of data points we currently have, so we must narrow these down. Let's run a correlation analysis to identify the most important variables:

```{r}
# Calculate correlations of all predictors with the target variable
correlations <- sapply(bogota_dataset[, !names(bogota_dataset) %in% 'workers.geih'], function(x) {
  cor(x, bogota_dataset$workers.geih, use = "complete.obs")
})

# Sort correlations by absolute value in descending order to see the most significant correlations at the top
sorted_correlations <- sort(abs(correlations), decreasing = TRUE)

# Print the sorted correlations
print(sorted_correlations)
```

```{r}
# Selecting variables based on correlation threshold
# Ensure to include the target variable 'workers.geih' explicitly
selected_vars <- names(which(abs(correlations) > 0.5))
selected_vars <- c("workers.geih", selected_vars)  # Adding the target variable

# Creating a subset of the dataset with these variables
bogota_model_data <- bogota_dataset[, selected_vars]

# Check that 'workers.geih' is indeed in the subset
if(!"workers.geih" %in% names(bogota_model_data)) {
  stop("workers.geih is not included in the model data.")
}

# Checking for multicollinearity with the Variance Inflation Factor (VIF)
library(car)
vif_model <- lm(workers.geih ~ ., data = bogota_model_data)
print(alias(vif_model))  # This function will show which terms have been aliased
vif_results <- vif(vif_model)
print(vif_results)

```


Correlation analysis between variables
```{r}
numeric_data <- final_data_employed %>% select_if(is.numeric)
head(numeric_data)
```


```{r}
cor_matrix <- cor(numeric_data, use = "complete.obs")  # 'use' argument handles missing values

# Convert the correlation matrix to a long dataframe
cor_data_long <- cor_matrix %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column("Variable1") %>% 
  pivot_longer(-Variable1, names_to = "Variable2", values_to = "Correlation")

# Remove self-correlations and ensure unique pairs
cor_data_long <- cor_data_long %>% 
  filter(Variable1 != Variable2) %>%
  distinct(Variable1, Variable2, .keep_all = TRUE)

cor_data_sorted <- cor_data_long %>% 
  mutate(AbsCorrelation = abs(Correlation)) %>% 
  arrange(desc(AbsCorrelation)) %>% 
  select(-AbsCorrelation)

cor_data_sorted
```

```{r}
# Optionally, filter for correlations above a certain threshold, e.g., |Correlation| > 0.5
cor_data_strong <- cor_data_sorted %>% 
  filter(abs(Correlation) > 0.5)

cor_data_strong
```











#Previous code
```{r}
# Initialize an empty list to store each dataset
datasets <- list()

# Loop through each worker type and create a separate dataset
for (type in worker_types) {
  # Filter df_analysis for the specific worker type
  datasets[[type]] <- df_analysis %>% filter(Concepto.geih == type)
}

# Now, datasets is a list where each element is a dataset corresponding to a worker type.
# For example, to access the dataset for "Ocupados", you can use datasets[["Ocupados"]]
df_analysis_employed <- datasets[["Ocupados"]]
```


Let's now check that the information makes sense / it was joined properly. We will be testing this on the information from the city of Barranquilla.
```{r}
df_analysis_baq <- df_analysis %>%
  filter(city == "BARRANQUILLA A.M.", Concepto == "Ocupados") %>%
  select(city, Concepto, date, workers, population_month, population_year, CPI, CPI_month_var, CPI_year_var, CPI_year_to_date_var)

# Round the column of workers so they are integers
df_analysis_baq$workers <- round(df_analysis_baq$workers,0)
df_analysis_baq
```


```{r}
# Ensure that 'date' is in Date format for plotting
df_analysis_baq$date <- as.Date(paste0(df_analysis_baq$date, "-01"), format = "%Y-%m-%d")

# Plot date vs workers, population_month, population_year
ggplot(df_analysis_baq) +
  geom_line(aes(x = date, y = workers, color = "Workers")) +
  geom_line(aes(x = date, y = population_month, color = "Population Month")) +
  geom_line(aes(x = date, y = population_year, color = "Population Year")) +
  labs(title = "Date vs Workers, Population Month, Population Year",
       x = "Date",
       y = "Count") +
  scale_color_manual(values = c("Workers" = "blue", "Population Month" = "green", "Population Year" = "red")) +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Plot date vs CPI
ggplot(df_analysis_baq) +
  geom_line(aes(x = date, y = CPI, color = "CPI")) +
  labs(title = "Date vs CPI",
       x = "Date",
       y = "CPI") +
  scale_color_manual(values = c("CPI" = "purple")) +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

# Plot date vs CPI_month_var, CPI_year_var, CPI_year_to_date_var
ggplot(df_analysis_baq) +
  geom_line(aes(x = date, y = CPI_month_var, color = "CPI Month Variation")) +
  geom_line(aes(x = date, y = CPI_year_var, color = "CPI Year Variation")) +
  geom_line(aes(x = date, y = CPI_year_to_date_var, color = "CPI Year to Date Variation")) +
  labs(title = "Date vs CPI Variations",
       x = "Date",
       y = "CPI Variation") +
  scale_color_manual(values = c("CPI Month Variation" = "orange", "CPI Year Variation" = "pink", "CPI Year to Date Variation" = "cyan")) +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

```
Let's fit some simple prediction models, but more research and data cleaning most be done in order to identify the best approach to our model. 

```{r}
# Split the data into training and test sets
train <- df_analysis_baq[df_analysis_baq$date < "2023-01-01", ]
test <- df_analysis_baq[df_analysis_baq$date >= "2023-01-01", ]

# Build the univariate model using the historical 'workers' data
uni_model <- auto.arima(train$workers)

# Forecast the 'workers' for 2024
uni_forecast <- forecast(uni_model, h = nrow(test))

# Build the multivariate model
# Note: You need to check for stationarity and co-integration among variables for an accurate multivariate model.
multi_model <- lm(workers ~ population_month + population_year + CPI + CPI_month_var + CPI_year_var + CPI_year_to_date_var, data = train)

# Forecast 'workers' for 2024 using the multivariate model
# For multivariate models, we don't have a direct function like forecast(), so we have to predict manually.
test$predicted_workers <- predict(multi_model, newdata = test)

# Evaluate the models
# For the univariate model
accuracy(uni_forecast, test$workers)

# For the multivariate model
multi_accuracy <- with(test, mean((workers - predicted_workers)^2)) # Mean Squared Error

# Visualization
plot(test$date, test$workers, type = "l", col = "blue", xlab = "Date", ylab = "Workers", main = "Actual vs Predicted Workers for Barranquilla in 2024")
lines(test$date, uni_forecast$mean, col = "red")
lines(test$date, test$predicted_workers, col = "black")
legend("topleft", legend = c("Actual", "Univariate", "Multivariate"), col = c("blue", "red", "black"), lty = 1)
```
- ME (Mean Error): Measures the average forecast error. Values close to 0 are preferable.
- RMSE (Root Mean Squared Error): Gives the standard deviation of the residuals, measuring how far the data points are from the model's predicted values. Lower values indicate a better fit.
- MAE (Mean Absolute Error): Similar to RMSE but uses absolute differences. It's easier to interpret than RMSE as it's on the same scale as the data.
- MPE (Mean Percentage Error): Measures the average percentage error. This helps understand the error in terms of percentage.
- MAPE (Mean Absolute Percentage Error): Like MPE but uses absolute values. This is useful for comparing the forecast performance across different datasets.
- MASE (Mean Absolute Scaled Error): Compares the MAE to the MAE of a naïve benchmark model. Values less than one indicate a model performing better than the naïve model.
- ACF1 (First Autocorrelation of Errors): Measures the correlation between the forecast errors and their lag. Values close to 0 indicate good model fit.


